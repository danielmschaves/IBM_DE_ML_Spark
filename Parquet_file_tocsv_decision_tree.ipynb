{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2a70ab4-affe-47f4-b214-e3ff1b873398",
   "metadata": {},
   "source": [
    "# Training a Random Forest Model and Data Conversion with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0762848-0638-43e7-909e-101dba4b258e",
   "metadata": {},
   "source": [
    "In this project, we demonstrate how to train a Random Forest model using Apache Spark. As a part of the process, we will also convert a Parquet file to CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c9a36e-48f7-47ac-9048-c6d6dc56739d",
   "metadata": {},
   "source": [
    "## Project Structure\n",
    "The project is divided into two main parts:\n",
    "\n",
    "- Converting a Parquet file to CSV: This part involves reading in a Parquet file, and converting it into a more common and easily readable CSV format using Apache Spark.\n",
    "\n",
    "- Training a Random Forest Model: In this part, we utilize the CSV data to train a Random Forest model. The model will be exported in PMML format, which is a standard format that allows for the representation of trained machine learning models.\n",
    "\n",
    "Now, let's dive into each part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dded5473-84b8-4181-b7e4-6ad94a14d63d",
   "metadata": {},
   "source": [
    "### Installation and Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e2ce23-5717-4231-8d03-af3c8f93c904",
   "metadata": {},
   "source": [
    "Before starting, we need to install the necessary packages. The installation depends on the Python version. Let's start by identifying the Python version and installing the necessary packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2244a82-cf20-451c-ae91-148e87c7de67",
   "metadata": {},
   "source": [
    "#### Read in parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacaff52-0e47-45dc-a3ca-19e410dc56d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "export version=`python --version |awk '{print $2}' |awk -F\".\" '{print $1$2}'`\n",
    "\n",
    "echo $version\n",
    "\n",
    "if [ $version == '36' ] || [ $version == '37' ]; then\n",
    "    echo 'Starting installation...'\n",
    "    pip3 install pyspark==2.4.8 wget==3.2 pyspark2pmml==0.5.1 > install.log 2> install.log\n",
    "    if [ $? == 0 ]; then\n",
    "        echo 'Please <<RESTART YOUR KERNEL>> (Kernel->Restart Kernel and Clear All Outputs)'\n",
    "    else\n",
    "        echo 'Installation failed, please check log:'\n",
    "        cat install.log\n",
    "    fi\n",
    "elif [ $version == '38' ] || [ $version == '39' ]; then\n",
    "    pip3 install pyspark==3.1.2 wget==3.2 pyspark2pmml==0.5.1 > install.log 2> install.log\n",
    "    if [ $? == 0 ]; then\n",
    "        echo 'Please <<RESTART YOUR KERNEL>> (Kernel->Restart Kernel and Clear All Outputs)'\n",
    "    else\n",
    "        echo 'Installation failed, please check log:'\n",
    "        cat install.log\n",
    "    fi\n",
    "else\n",
    "    echo 'Currently only python 3.6, 3.7 , 3.8 and 3.9 are supported, in case you need a different version please open an issue at https://github.com/IBM/claimed/issues'\n",
    "    exit -1\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47916c9-8cbe-4b83-8b9b-dcba1d2ee119",
   "metadata": {},
   "source": [
    "#### Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d5ff207-c6a2-41ec-8e51-838a1a70ff49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark2pmml import PMMLBuilder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "import logging\n",
    "import shutil\n",
    "import site\n",
    "import sys\n",
    "import wget\n",
    "import glob\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2712335f-4dd0-47a5-bbaa-a0eb65bb2b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# source path and file name (default: data.parquet)\n",
    "data_parquet = os.environ.get('data_parquet', 'data.parquet')\n",
    "\n",
    "# destination path and parquet file name (default: data.csv)\n",
    "output_data_csv = os.environ.get('output_data_csv', 'data.csv')\n",
    "\n",
    "# url of master (default: local mode)\n",
    "master = os.environ.get('master', \"local[*]\")\n",
    "\n",
    "# temporal data storage for local execution\n",
    "data_dir = os.environ.get('data_dir', '../../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087df94d-00a9-4ed1-bf61-ff569244761a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5464a624-e6e1-429b-a0f0-1870549049a7",
   "metadata": {},
   "source": [
    "### Parquet to CSV Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e699b8b-a2e4-4856-b2be-430ac425413c",
   "metadata": {},
   "source": [
    "Next, we read in the parquet file, convert it to CSV, and save the result. The script first checks if the CSV file already exists. If it does, the script skips the conversion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5b88f84-0343-485d-bf84-326f6f3d6fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip:\n",
    "    sc = SparkContext.getOrCreate(SparkConf().setMaster(master))\n",
    "    spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "796c8c1b-3759-4c69-ba73-173ca40282f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip:\n",
    "    df = spark.read.parquet(data_dir + data_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ee049ec-77d6-44d0-9dfe-616c96a29112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip:\n",
    "    if os.path.exists(data_dir + output_data_csv):\n",
    "        shutil.rmtree(data_dir + utput_data_csv)\n",
    "    df.coalesce(1).write.option(\"header\", \"true\").csv(data_dir + output_data_csv)\n",
    "    file = glob.glob(data_dir + output_data_csv + '/part-*')\n",
    "    shutil.move(file[0], data_dir + output_data_csv + '.tmp')\n",
    "    shutil.rmtree(data_dir + output_data_csv)\n",
    "    shutil.move(data_dir + output_data_csv + '.tmp', data_dir + output_data_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5380fd23-39e5-4761-a2d6-045d14f73dc9",
   "metadata": {},
   "source": [
    "### Training a Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0471355b-bf3e-4423-8c11-ada63355b71e",
   "metadata": {},
   "source": [
    "Once we have the CSV file, we can use it to train a Random Forest model. First, we download the correct version of the JPMMl library, which is required to export the trained model to PMML format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4f8eadc-8101-4100-8fa7-4461a9141fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if sys.version[0:3] == '3.9':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.7.2/'\n",
    "           'jpmml-sparkml-executable-1.7.2.jar')\n",
    "    wget.download(url)\n",
    "    shutil.copy('jpmml-sparkml-executable-1.7.2.jar',\n",
    "                site.getsitepackages()[0] + '/pyspark/jars/')\n",
    "elif sys.version[0:3] == '3.8':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.7.2/'\n",
    "           'jpmml-sparkml-executable-1.7.2.jar')\n",
    "    wget.download(url)\n",
    "    shutil.copy('jpmml-sparkml-executable-1.7.2.jar',\n",
    "                site.getsitepackages()[0] + '/pyspark/jars/')\n",
    "elif sys.version[0:3] == '3.7':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.5.12/'\n",
    "           'jpmml-sparkml-executable-1.5.12.jar')\n",
    "    wget.download(url)\n",
    "elif sys.version[0:3] == '3.6':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.5.12/'\n",
    "           'jpmml-sparkml-executable-1.5.12.jar')\n",
    "    wget.download(url)\n",
    "else:\n",
    "    raise Exception('Currently only python 3.6 , 3.7, 3,8 and 3.9 is supported, in case '\n",
    "                    'you need a different version please open an issue at '\n",
    "                    'https://github.com/IBM/claimed/issues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b007b4-bd8a-4fe0-8299-02d4bc8701d1",
   "metadata": {},
   "source": [
    "With the necessary libraries installed, we can now define additional parameters and initialize our Spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f37775fc-b202-4b89-b5e0-c7c4f11de3e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " # input file name (data.csv)\n",
    "data_csv = os.environ.get('data_csv', 'data.csv') \n",
    "\n",
    "# url of master (default: local mode)\n",
    "master = os.environ.get('master', \"local[*]\")\n",
    "\n",
    "# model output file name\n",
    "model_target = os.environ.get('model_target', \"model.xml\")\n",
    "\n",
    "# temporary directory for data\n",
    "data_dir = os.environ.get('data_dir', '../../data/') \n",
    "\n",
    "# input columns to consider\n",
    "input_columns = os.environ.get('input_columns', '[\"x\", \"y\", \"z\"]') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d82622ad-2956-47a2-9c03-ed12aa3e7cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = list(\n",
    "    map(lambda s: re.sub('$', '\"', s),\n",
    "        map(\n",
    "            lambda s: s.replace('=', '=\"'),\n",
    "            filter(\n",
    "                lambda s: s.find('=') > -1 and bool(re.match(r'[A-Za-z0-9_]*=[.\\/A-Za-z0-9]*', s)),\n",
    "                sys.argv\n",
    "            )\n",
    "    )))\n",
    "\n",
    "for parameter in parameters:\n",
    "    logging.warning('Parameter: ' + parameter)\n",
    "    exec(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e6b6532-8b52-440c-8707-6346800a055b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/19 01:31:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/19 01:31:52 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/06/19 01:31:52 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/06/19 01:31:52 WARN util.Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/06/19 01:31:52 WARN util.Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/06/19 01:31:52 WARN util.Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setMaster(master)\n",
    "\n",
    "#if sys.version[0:3] == '3.6' or sys.version[0:3] == '3.7':\n",
    "conf.set(\"spark.jars\", 'jpmml-sparkml-executable-1.5.12.jar')\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = sqlContext.sparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e070f869-e3bd-4f63-b42f-47fbf294624f",
   "metadata": {},
   "source": [
    "After initializing the Spark context, we can read in the CSV file and preprocess the data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5bac9dd-fbeb-4fd2-904b-52150af99674",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#df = spark.read.csv(data_dir + data_csv)\n",
    "df = spark.read.option(\"header\", \"true\").csv(data_dir + data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e12115d-b727-4f90-b654-77488b683e26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# register a corresponding query table\n",
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34f4e0a0-4d74-4c51-a13a-4e98732e4b75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "df = df.withColumn(\"x\", df.x.cast(DoubleType()))\n",
    "df = df.withColumn(\"y\", df.y.cast(DoubleType()))\n",
    "df = df.withColumn(\"z\", df.z.cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc9eae2a-a05f-42df-9c82-e9cba2374658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "splits = df.randomSplit([0.8, 0.2])\n",
    "df_train = splits[0]\n",
    "df_test = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597dbb44-0c03-4e5a-b0ee-83927eabb20a",
   "metadata": {},
   "source": [
    "We proceed by training the Random Forest model and saving it in PMML format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eed2b975-7770-4c7b-b704-bfbb4f6cdbe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=eval(input_columns),\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "normalizer = MinMaxScaler(inputCol=\"features\", outputCol=\"features_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b64ec6ca-1723-438e-8630-89732a0ee3e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees 10 Maximum Depth 5 seed 1 Prediction 0.44228418971337163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees 10 Maximum Depth 7 seed 1 Prediction 0.4648783038892975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees 20 Maximum Depth 5 seed 1 Prediction 0.44238486901902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 200:==================================================>    (25 + 2) / 27]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees 20 Maximum Depth 7 seed 1 Prediction 0.46930819333782664\n",
      "Best parameters: {'numTrees': 20, 'maxDepth': 7, 'seed': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "best_predic = 0\n",
    "best_params = {'numTrees': None, 'maxDepth': None, 'seed': None}\n",
    "\n",
    "for curNTrees in [10, 20]:\n",
    "    for curMaxDepth in [5, 7]:\n",
    "        rf = RandomForestClassifier(numTrees=curNTrees, maxDepth=curMaxDepth, seed=1)\n",
    "        pipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer, rf])\n",
    "        model = pipeline.fit(df_train)\n",
    "        prediction = model.transform(df_train)\n",
    "\n",
    "        binEval = MulticlassClassificationEvaluator(). \\\n",
    "            setMetricName(\"accuracy\"). \\\n",
    "            setPredictionCol(\"prediction\"). \\\n",
    "            setLabelCol(\"label\")\n",
    "        predic = binEval.evaluate(prediction)\n",
    "\n",
    "        if predic > best_predic:\n",
    "            best_predic = predic\n",
    "            best_params = {'numTrees': curNTrees, 'maxDepth': curMaxDepth, 'seed': seed}\n",
    "\n",
    "        print('Number of trees', curNTrees, 'Maximum Depth', curMaxDepth, 'seed', seed, 'Prediction', predic)\n",
    "\n",
    "print('Best parameters:', best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22ad9e1b-ac01-4575-9b9e-6c3c31be3bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/resources/labs/BD0231EN/component-library-1/component-library/deploy/../../data/model.xml'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmmlBuilder = PMMLBuilder(sc, df_train, model)\n",
    "pmmlBuilder.buildFile(data_dir + model_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6342e20a-a405-42d8-9f81-5761efa67406",
   "metadata": {},
   "source": [
    "That's it! You have now converted a Parquet file to CSV, and trained a Random Forest model using Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25612e38-e412-48c9-9713-4845e08620bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
